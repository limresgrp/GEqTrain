# general
root: results/debug_runs
run_name: run

seed: 0
dataset_seed: 0
append: true
default_dtype: float32
device: cuda:0

# -- network --
model_builders:
  - geqtrain.model.Model
  # - geqtrain.model.Allegro

# cutoffs
r_max: 6.0
avg_num_neighbors: 50.0

# radial basis
PolynomialCutoff_p: 12
num_basis: 8

# symmetry
l_max: 2
parity: o3_full

# layers:
num_layers: 2
env_embed_multiplicity: 64

env_embed_mlp_latent_dimensions: [64, 64]
env_embed_mlp_nonlinearity: ssp

two_body_latent_mlp_latent_dimensions: [64, 128, 256]
two_body_latent_mlp_nonlinearity: ssp

latent_mlp_latent_dimensions: [256, 256]
latent_mlp_nonlinearity: ssp

product_correlation: 2   # Maximum correlation order of tensor product expansion

# - end layers -

# default of pre_pooling_readout readout
pre_pooling_readout_mlp_latent_dimensions: [256, 256]

# Final MLP to go from latent space to node output:
readout_mlp_latent_dimensions: [256, 256]
readout_mlp_nonlinearity: null

out_irreps: '1x0e'

# -- data --
# n_train: [1,1,1,1,1,1,1,1,0,0] # out of train data, which idxs to use to train and which to use to val

dataset_list:
  - dataset: npz
    dataset_input: /storage_common/nobilm/qm_npz/small_sample_train/
    key_mapping:
      coords: pos # the blue keywords are the kwords present in the npz, they are mapped to the red kwords that are present/used in code
      atom_types: node_types
      graph_labels: mu # mu kword used in code, since custom it has to be registered; for custom values it must be defined below if it is a node_fields, edge_fields, graph_fields, long_fields
    npz_fixed_field_keys:
    - node_types

validation_dataset_list:
  - validation_dataset: npz
    validation_dataset_input: /storage_common/nobilm/qm_npz/small_sample_test/
    key_mapping:
      coords: pos # the blue keywords are the kwords present in the npz, they are mapped to the red kwords that are present/used in code
      atom_types: node_types
      graph_labels: mu # mu kword used in code, since custom it has to be registered; for custom values it must be defined below if it is a node_fields, edge_fields, graph_fields, long_fields
    npz_fixed_field_keys:
    - node_types

# node_fields:
  # - node_output # da registrare in atom dict la lista dei label_name
# edge_fields:
graph_fields:
  - mu
# long_fields:

num_types: 9

#############################################################################################################################

# loss function
loss_coeffs:
  - mu: # target -? key taken from dat dict of batch
    - 1. # scalar that can be used to weight/rescale impact of this loss onto gradients
    - MSELoss
    - ignore_nan: true # wheter or not to ignore gradients if they are nan wrt this loss
      ignore_zeroes: true

metrics_components:
  - - mu
    - mae
    - PerSpecies: True
      ignore_nan: True
      functional: L1Loss

### I think this is wrong
report_init_validation: false

#############################################################################################################################

# logging
wandb: false
wandb_project: example
verbose: info
log_batch_freq: 20

# training
batch_size: 32
validation_batch_size: 32

# Configure maximum batch sizes to avoid GPU memory errors. These parameters have to be configured according to your GPU RAM #

batch_max_atoms: 3000               # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

#############################################################################################################################

max_epochs: 10000
learning_rate: 1.0e-3
train_val_split: random
shuffle: true
metrics_key: validation_loss

# use an exponential moving average of the weights
use_ema: true
ema_decay: 0.99
ema_use_num_updates: true

# optimizer
optimizer_name: Adam
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.

# lr scheduler, drop lr if no improvement for tot epochs
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 10
lr_scheduler_factor: 0.5

early_stopping_lower_bounds:
  LR: 1.0e-5

early_stopping_patiences:
  validation_loss: 100