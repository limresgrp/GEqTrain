# general
root: results/qm9_mu
run_name: testing
experiment_description: test

seed: 0
dataset_seed: 0
append: true
default_dtype: float32
device: cuda:3

# -- network --
model_builders:
  - geqtrain.model.Model

# cutoffs
r_max: 50.0
avg_num_neighbors: 18.02485649425595

# radial basis
PolynomialCutoff_p: 12
num_basis: 8

# symmetry
l_max: 2
parity: o3_full

# layers:
num_layers: 1
env_embed_multiplicity: 341

env_embed_mlp_latent_dimensions: [64, 64]
env_embed_mlp_nonlinearity: ssp

two_body_latent_mlp_latent_dimensions: [64, 128, 256]
two_body_latent_mlp_nonlinearity: ssp

latent_mlp_latent_dimensions: [256, 256]
latent_mlp_nonlinearity: ssp

product_correlation: 2   # Maximum correlation order of tensor product expansion

# head
head_function_mlp_latent_dimensions: [128]
head_function_mlp_nonlinearity: null
head_function_mlp_has_bias: true

# - end layers -

# default of pre_pooling_readout readout
# pre_pooling_readout_mlp_latent_dimensions: [256, 256]

per_node_features_use_attention: false

# Final MLP to go from latent space to node output:
readout_mlp_latent_dimensions: [256, 256]
readout_mlp_nonlinearity: null

out_irreps: '12x0e'

# dataset creation source data:
# https://www.tensorflow.org/datasets/catalog/qm9
# used the one with lowest ammout of train data
dataset_list:
  - dataset: npz
    dataset_input:  /storage_common/nobilm/qm_npz/TEST_FULL_LABELS/
    key_mapping:
      coords: pos # the blue keywords are the kwords present in the npz, they are mapped to the red kwords that are present/used in code
      atom_types: node_types
      graph_labels: graph_labels # mu kword used in code, since custom it has to be registered; for custom values it must be defined below if it is a node_fields, edge_fields, graph_fields, long_fields
    npz_fixed_field_keys:
    - node_types

validation_dataset_list:
  - validation_dataset: npz
    validation_dataset_input:  /storage_common/nobilm/qm_npz/TEST_FULL_LABELS/
    key_mapping:
      coords: pos
      atom_types: node_types
      graph_labels: graph_labels
    npz_fixed_field_keys:
    - node_types

graph_fields:
  - graph_labels

num_types: 5

#############################################################################################################################

# loss function
loss_coeffs:
  - graph_labels: # target -? key taken from dat dict of batch
    - 1. # scalar that can be used to weight/rescale impact of this loss onto gradients
    - MSELoss # func initializzato
    - ignore_nan: true # wheter or not to ignore gradients if they are nan wrt this loss
      ignore_zeroes: true

# metrics report
metrics_components:
  - graph_labels: # key
    - geqtrain.train._loss.PerLabelLoss # func
    - PerLabel: True
      # functional: L1Loss # which already is def val


### I think this is wrong
report_init_validation: false

#############################################################################################################################

# logging
wandb: false
wandb_project: qm9_mu_10k_good_val
verbose: info
log_batch_freq: 20

# training
batch_size: 128
validation_batch_size: 128

# Configure maximum batch sizes to avoid GPU memory errors. These parameters have to be configured according to your GPU RAM #

batch_max_atoms: 3000               # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

#############################################################################################################################

max_epochs: 10000
learning_rate: 1.0e-3
train_val_split: random
shuffle: true
metrics_key: validation_loss

# use an exponential moving average of the weights
use_ema: true
ema_decay: 0.99
ema_use_num_updates: true

# optimizer
optimizer_name: Adam
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.

# lr scheduler, drop lr if no improvement for tot epochs
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 10
lr_scheduler_factor: 0.5

early_stopping_lower_bounds:
  LR: 1.0e-5

early_stopping_patiences:
  validation_loss: 10

sanitize_gradients: false